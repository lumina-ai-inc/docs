---
title: Best Practices
description: Using the Parse feature for different cases
---

This guide covers best practices for configuring the Parse feature to handle a variety of use cases and requirements.

## Extended Context: Handling Distant Legends

<Frame>Extended context example image</Frame>

Elements like tables or charts might rely on context from other parts of the page. For example, a chart's legend could be located in a different corner of the document that isn't picked up when the cropped chart is sent to a VLM.

For these scenarios, the best practice is to enable `extended_context`. This provides the VLM with the full page image with the cropped segment as context.

Hereâ€™s how to enable it for `Table` and `Picture` segments:

<CodeGroup>
  ```python Python
  from chunkr_ai import Chunkr
  from chunkr_ai.types.tasks.parse_create_params import (
      SegmentProcessing,
      SegmentProcessingTable,
      SegmentProcessingPicture
  )

  client = Chunkr(api_key="your_api_key")

  # Parse with extended context for tables and pictures
  task = client.tasks.parse.create(
      file="path/to/file.pdf",
      segment_processing=SegmentProcessing(
          Table=SegmentProcessingTable(
              extended_context=True
          ),
          Picture=SegmentProcessingPicture(
              extended_context=True
          )
      )
  )

  # Wait for completion
  task = client.tasks.get(task_id=task.task_id, wait_for_completion=True)
  ```

  ```typescript TypeScript
  import Chunkr from 'chunkr-ai';

  const client = new Chunkr({ apiKey: 'your_api_key' });

  // Parse with extended context for tables and pictures
  const task = await client.tasks.parse.create({
      file: 'path/to/file.pdf',
      segment_processing: {
          Table: {
              extended_context: true
          },
          Picture: {
              extended_context: true
          }
      }
  });

  // Wait for completion
  const completedTask = await client.tasks.get(task.task_id, {
      wait_for_completion: true
  });
  ```

  ```bash cURL
  curl -X POST https://api.chunkr.ai/api/v1/tasks/parse \
    -H "Authorization: YOUR_API_KEY" \
    -H "Content-Type: application/json" \
    -d '{
        "file": "https://example.com/document.pdf",
        "segment_processing": {
            "Table": {
                "extended_context": true
            },
            "Picture": {
                "extended_context": true
            }
        }
    }'
  ````

</CodeGroup>
<Tip>
  Extended context adds some latency.
</Tip>
---

## Pre-signed URLs vs. Base64

By default, Chunkr provides access to generated files (like images, the input file, or PDF) via pre-signed URLs. These are secure but temporary, expiring after 10 minutes. This is great for immediate access, but not for long-term storage or if your system needs to access the files later.

For durable, long-term access, the recommended approach is to retrieve file assets as base64-encoded strings. This embeds the file data directly in the task response, which you can then store permanently.

To do this, set the `base64_urls=True` parameter when fetching a task:

<CodeGroup>
  ```python Python
  from chunkr_ai import Chunkr

  client = Chunkr(api_key="your_api_key")

  # Get task with base64-encoded URLs instead of pre-signed URLs
  task = client.tasks.get(task_id="task_123", base64_urls=True)
  ```

  ```typescript TypeScript
  import Chunkr from 'chunkr-ai';

  const client = new Chunkr({ apiKey: 'your_api_key' });

  // Get task with base64-encoded URLs instead of pre-signed URLs
  const task = await client.tasks.get('task_123', { base64_urls: true });
  ```

  ```bash cURL
  curl -X GET "https://api.chunkr.ai/api/v1/tasks/{task_id}?base64_urls=true" \
    -H "Authorization: YOUR_API_KEY"
  ````

</CodeGroup>

---

## Optimizing for speed

The most significant factor affecting processing time is the VLM used in the `llm_processing` configuration. By default, Chunkr uses `chunkr-parse-1-thinking`, which provides the highest quality results but can be slower.

To significantly increase processing speed without taking a large hit on accuracy, you can switch to our non-thinking variant. We recommend setting `chunkr-parse-1` as the main model.

<CodeGroup>
  ```python Python
  from chunkr_ai import Chunkr
  from chunkr_ai.types.tasks.parse_create_params import (
      LlmProcessing,
      LlmProcessingFallbackStrategyModel
  )

  client = Chunkr(api_key="your_api_key")

  # Optimized for speed
  task = client.tasks.parse.create(
      file="path/to/file.pdf",
      llm_processing=LlmProcessing(
          llm_model_id="chunkr-parse-1",
          fallback_strategy=LlmProcessingFallbackStrategyModel(
              model="chunkr-parse-1"
          )
      )
  )

  # Wait for completion
  task = client.tasks.get(task_id=task.task_id, wait_for_completion=True)
  ```

  ```typescript TypeScript
  import Chunkr from 'chunkr-ai';

  const client = new Chunkr({ apiKey: 'your_api_key' });

  // Optimized for speed
  const task = await client.tasks.parse.create({
      file: 'path/to/file.pdf',
      llm_processing: {
          llm_model_id: 'chunkr-parse-1',
          fallback_strategy: {
              model: 'chunkr-parse-1'
          }
      }
  });

  // Wait for completion
  const completedTask = await client.tasks.get(task.task_id, {
      wait_for_completion: true
  });
  ```

  ```bash cURL
  curl -X POST https://api.chunkr.ai/api/v1/tasks/parse \
    --header "Authorization: YOUR_API_KEY" \
    --header "Content-Type: application/json" \
    --data '{
      "file": "https://example.com/document.pdf",
      "llm_processing": {
          "llm_model_id": "chunkr-parse-1",
          "fallback_strategy": {
              "model": "chunkr-parse-1"
          }
      }
    }'
  ````

</CodeGroup>


