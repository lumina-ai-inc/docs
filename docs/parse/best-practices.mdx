---
title: Best Practices
description: Using the Parse feature for different cases
---

This guide covers best practices for configuring the Parse feature to handle a variety of use cases and requirements.

## Extended Context: Handling Distant Legends

<Frame>Extended context example image</Frame>

Elements like tables or charts might rely on context from other parts of the page. For example, a chart's legend could be located in a different corner of the document that isn't picked up when the cropped chart is sent to a VLM.

For these scenarios, the best practice is to enable `extended_context`. This provides the VLM with the full page image with the cropped segment as context.

Hereâ€™s how to enable it for `Table` and `Picture` segments:

<CodeGroup>
  ```python Python
  from chunkr_ai import Chunkr
  from chunkr_ai.models import (
      Configuration,
      GenerationConfig,
      SegmentProcessing,
  )

chunkr = Chunkr()

config = Configuration(
segment_processing=SegmentProcessing(
Table=GenerationConfig(
extended_context=True
),
Picture=GenerationConfig(
extended_context=True
)
)
)

chunkr.upload("path/to/file", config)

````

```bash cURL
curl -X POST https://api.chunkr.ai/api/v1/task/parse \
  -H "Authorization: YOUR_API_KEY" \
  -F file=@path/to/file \
  -F 'segment_processing={
      "Table": {
        "extended_context": true
      },
      "Picture": {
        "extended_context": true
      }
    };type=application/json'
````

</CodeGroup>
<Tip>
  Extended context adds some latency.
</Tip>
---

## Pre-signed URLs vs. Base64

By default, Chunkr provides access to generated files (like images, the input file, or PDF) via pre-signed URLs. These are secure but temporary, expiring after 10 minutes. This is great for immediate access, but not for long-term storage or if your system needs to access the files later.

For durable, long-term access, the recommended approach is to retrieve file assets as base64-encoded strings. This embeds the file data directly in the task response, which you can then store permanently.

To do this, set the `base64_urls=True` parameter when fetching a task:

<CodeGroup>
  ```python Python
  from chunkr_ai import Chunkr

chunkr = Chunkr()

# Get task with base64-encoded URLs instead of pre-signed URLs

task = chunkr.get_task("task_123", base64_urls=True)

````

```bash cURL
curl -X GET "https://api.chunkr.ai/api/v1/task/{task_id}?base64_urls=true" \
  -H "Authorization: YOUR_API_KEY"
````

</CodeGroup>

---

## Optimizing for speed

The most significant factor affecting processing time is the VLM used in the `llm_processing` configuration. By default, Chunkr uses `chunkr-parse-1-thinking`, which provides the highest quality results but can be slower.

To significantly increase processing speed without takign a large hit on accuracy, you can to our non-thinking variant. We recommend setting `chunkr-parse-1` as the main model.

<CodeGroup>
  ```python Python
  from chunkr_ai import Chunkr
  from chunkr_ai.models import Configuration, LlmProcessing, FallbackStrategy

chunkr = Chunkr()

# Optimized for speed

config = Configuration(
llm_processing=LlmProcessing(
llm_model_id="chunkr-parse-1",
fallback_strategy=FallbackStrategy.model("chunkr-parse-1"
)
)

task = chunkr.upload("path/to/file", config)

````

```bash cURL
curl -X POST https://api.chunkr.ai/api/v1/task/parse \
  --header "Authorization: YOUR_API_KEY" \
  --header "Content-Type: application/json" \
  --data '{
    "file": "base64_or_url_to_file",
    "file_name": "document.pdf",
    "llm_processing": {
        "llm_model_id": "chunkr-parse-1",
        "fallback_strategy": null
    }
  }'
````

</CodeGroup>


